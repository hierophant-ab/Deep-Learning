# Deep-Learning SOC 2025

This project delves into the world of Deep learning encompassing both core Machine learning Algorithms like Linear and Logistic Regresssion as well as Artificial Neural Networks and Convolutional Neural Networks.

## Progress so far
### Week 1
Week 1 was all about mastering python basics that included if-else statements, loops, different data structures and functions. Further it delves into essential python libraries like numpy, pandas and matplotlib giving basic understanding of each of them. These are the building blocks of what's coming in subsequent weeks.

### Week 2 
Week 2 introduced one of the most important machine learning algorithm **Logistic Regression**. In this week, we learnt binary classification, semantics of logistic regression- forward propagation, back propagation, cost function and gradient descent.<br />
**Forward propagation**- Forward propagation is the process that transforms inputs into meaningful outputs, forming the basis for both prediction and learning. <br />
Let X be the input, W be the weights and b be the bias then, <br />
&emsp;Z= WX+b <br />
&emsp;A= g(Z)        where g(x) is called activation function<br />
Activation function used here is callled sigmoid function. Activation functions add non-linearity to the input data.<br />
**Back Propagation-** It enables the network to learn from its mistakes by efficiently computing how each weight in the network contributed to the final error, and then updating those weights to improve future predictions. It is the most important and most math heavy part of machine learning. <br />
**Cost Function**- 1/m*(summation(Y*log(1-A)+(1-Y)*log(1-A))<br /> This cost function is also called **Categorical Cross Entropy Loss**.<br />

### Week 3
Week 3 delved into artificial neural network, the method that powers the modern AI Systems. It includes layers and neurons each with their associated weights and biases. A neural network is capable of immage recognition, NLP, RL, computer vision etc.<br /> We learnt about neural network architecture, concept of hidden layers and 4 fundamental equations of back propagation. Concepts of forward propagation, cost function, gradient descent are carry forward into this week.<br /><br />
&emsp;    ![image](https://github.com/user-attachments/assets/ce2a4072-a524-427c-a8ae-6d533b9dadbb)<br />

### Week 4
Made a mini-project on digit classification using fully connected neural network. Dataset used is MNIST dataset.<br />
Achieved an accuracy of 100% on training data and 98.14% on test data.










